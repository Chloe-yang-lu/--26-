##1. Data Collection-Web Scraping

#necessary environment
!pip install praw==7.7.1
!pip install pandas==2.2.2
!pip install numpy==1.24.2

import praw
import pandas as pd
import numpy as np
import time

#Reddit API setting
client_id = "b0PjlScu76ntilQiWfB_LA"
client_secret = "6yRvmF2e10JQimcjELdHc30QVWrWPw"
user_agent = "Ethio"

#connect to Reddit
reddit = praw.Reddit(
    client_id=client_id,
    client_secret=client_secret,
    user_agent=user_agent
)

#Scraping data
subreddit = reddit.subreddit("gaming")  # only select "gaming" subpart
query = "Assassin's Creed Mirage OR Assassin's Creed Shadows OR AC Mirage OR AC Shadows"
time_filter = "week"
limit = 5000   # scrape 5000 posts

#Store data
data = []
for post in subreddit.search(query, limit=limit):
    time.sleep(2)  # give sleep time

    # scrape posts
    post_id = post.id
    post_title = post.title
    post_text = post.selftext
    post_score = post.score
    post_num_comments = post.num_comments
    post_url = post.url
    post_created = post.created_utc

    # scrape comments
    comments = []
    try:
        post.comments.replace_more(limit=0)
        for comment in post.comments.list():
            comments.append(comment.body)
    except Exception as e:
        print(f"failed to scrape commentsï¼š{e}")

    # save into lists
    data.append({
        "post_id": post_id,
        "title": post_title,
        "content": post_text,
        "score": post_score,
        "num_comments": post_num_comments,
        "url": post_url,
        "created_utc": post_created,
        "comments": " || ".join(comments)  # append all comments into strings
    })


df = pd.DataFrame(data)
df.head()

# Convert to Dataframe and save to CSV
output_file = "AC_Shadow_Reddit_Data.csv"
df.to_csv(output_file, index=False, encoding="utf-8")
