##2. Data Processing-Text Preparation

#environment
!pip install nltk==3.9.1
!pip install regex

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

###2.1 Data Cleanning

#install matching pandas
import pandas as pd

#import dataset
df = pd.read_csv('/content/AC_Shadow_Reddit_Data.csv')

#check datastructure
print(df.head)
print('NaN content portion is:' + str(100*(df['content'].isna().sum() / len(df)))+'%')

#Replace NaN content with space
df['text'] = df['title'].fillna('') + ' ' + df['content'].fillna('') + ' ' + df['comments'].fillna('')

#Convert date
df['date'] = pd.to_datetime(df['created_utc'], unit='s')

# Only select necessary collumns
df = df[['score', 'num_comments', 'date', 'text']]

df.head()

###2.3 Text processing with NLP techniques

# Standardised abbreviations and slang
slang_dict = {
    "u": "you",
    "ur": "your",
    "thx": "thanks",
    "idk": "i don't know",
    "imo": "in my opinion",
    "btw": "by the way",
}

def replace_slang(text):
    words = text.split()
    new_words = [slang_dict[word] if word in slang_dict else word for word in words]
    return " ".join(new_words)

# replace slangs
df['clean_text'] = df['text'].apply(replace_slang)


# Text cleaning
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = re.sub(r'http\S+|www\S+', '', text)     # URL
    text = re.sub(r'<.*?>', '', text)               #  HTML tag
    text = re.sub(r'[^\w\s]', '', text)             # punctuation mark
    text = re.sub(r'\d+', '', text)                 # re number
    text = re.sub(r'\s+', ' ', text)
    text = text.lower().strip()
    return text


def tokenize_text(text):
    return word_tokenize(text)

# Remove stopwords
def remove_stopwords(tokens, custom_stopwords=None):
    stop_words = set(stopwords.words('english'))
    if custom_stopwords:
        stop_words.update(custom_stopwords)

    return [word for word in tokens if word not in stop_words]


def lemmatize_tokens(tokens):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in tokens]

# Union all processing function
def preprocess_text(text, custom_stopwords=None):
    cleaned_text = clean_text(text)
    tokens = tokenize_text(cleaned_text)
    tokens = lemmatize_tokens(tokens)
    tokens = remove_stopwords(tokens, custom_stopwords)
    return " ".join(tokens)


## Adjust customed Stopwords
'''always adjust stopwords according to Topic modelling, until results are idea.
'''
custom_stopwords = {'game', 'play', 'one', 'people', 'assassin', 'creed', 'shadow', 'ac','feel'}  # self defined stop words


#apply to each text
df['ready_text'] = df['clean_text'].apply(lambda x: preprocess_text(x, custom_stopwords))

#check cleaned text
df.head()


# Save processed text
df.to_csv('ready_text.csv', index=False)
